{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cattle Detection Dataset Analysis - Phase 1.1\n",
    "\n",
    "This notebook performs initial analysis of the cattle detection datasets downloaded from Roboflow.\n",
    "\n",
    "## Objectives\n",
    "1. Explore the structure of downloaded datasets\n",
    "2. Analyze image properties (dimensions, formats, etc.)\n",
    "3. Examine annotation quality and distribution\n",
    "4. Visualize sample images with bounding boxes\n",
    "5. Prepare data statistics for model training decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Current working directory: /home/nicolas-ubuntu/Desktop/repos/cattle/cattle-recognition/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/nicolas-ubuntu/Desktop/repos/cattle/cattle-recognition\n",
      "Data directory: /home/nicolas-ubuntu/Desktop/repos/cattle/cattle-recognition/data/detection\n",
      "Data directory exists: True\n",
      "\n",
      "Found 0 datasets:\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"detection\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Data directory exists: {DATA_DIR.exists()}\")\n",
    "\n",
    "# List all downloaded datasets\n",
    "if DATA_DIR.exists():\n",
    "    datasets = [d for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "    print(f\"\\nFound {len(datasets)} datasets:\")\n",
    "    for dataset in datasets:\n",
    "        print(f\"  - {dataset.name}\")\n",
    "else:\n",
    "    print(\"\\n❌ Data directory not found. Please run the download script first.\")\n",
    "    datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure(dataset_path):\n",
    "    \"\"\"Analyze the structure of a YOLO dataset.\"\"\"\n",
    "    dataset_info = {\n",
    "        'name': dataset_path.name,\n",
    "        'path': str(dataset_path),\n",
    "        'splits': {},\n",
    "        'total_images': 0,\n",
    "        'total_annotations': 0\n",
    "    }\n",
    "    \n",
    "    # Look for YOLO dataset structure (train, val, test folders)\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        split_path = dataset_path / split\n",
    "        if split_path.exists():\n",
    "            images_path = split_path / 'images'\n",
    "            labels_path = split_path / 'labels'\n",
    "            \n",
    "            if images_path.exists() and labels_path.exists():\n",
    "                image_files = list(images_path.glob('*.[jp][pn]g')) + list(images_path.glob('*.jpeg'))\n",
    "                label_files = list(labels_path.glob('*.txt'))\n",
    "                \n",
    "                dataset_info['splits'][split] = {\n",
    "                    'images': len(image_files),\n",
    "                    'labels': len(label_files),\n",
    "                    'images_path': str(images_path),\n",
    "                    'labels_path': str(labels_path)\n",
    "                }\n",
    "                \n",
    "                dataset_info['total_images'] += len(image_files)\n",
    "                dataset_info['total_annotations'] += len(label_files)\n",
    "    \n",
    "    # Look for data.yaml file\n",
    "    yaml_file = dataset_path / 'data.yaml'\n",
    "    if yaml_file.exists():\n",
    "        try:\n",
    "            with open(yaml_file, 'r') as f:\n",
    "                yaml_data = yaml.safe_load(f)\n",
    "            dataset_info['yaml_info'] = yaml_data\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read {yaml_file}: {e}\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Analyze all datasets\n",
    "dataset_analyses = []\n",
    "for dataset_path in datasets:\n",
    "    analysis = analyze_dataset_structure(dataset_path)\n",
    "    dataset_analyses.append(analysis)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset: {analysis['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total images: {analysis['total_images']}\")\n",
    "    print(f\"Total annotations: {analysis['total_annotations']}\")\n",
    "    \n",
    "    for split, info in analysis['splits'].items():\n",
    "        print(f\"  {split.capitalize()}: {info['images']} images, {info['labels']} labels\")\n",
    "    \n",
    "    if 'yaml_info' in analysis:\n",
    "        yaml_info = analysis['yaml_info']\n",
    "        if 'names' in yaml_info:\n",
    "            print(f\"  Classes: {yaml_info['names']}\")\n",
    "        if 'nc' in yaml_info:\n",
    "            print(f\"  Number of classes: {yaml_info['nc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Properties Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing image properties...\n"
     ]
    }
   ],
   "source": [
    "def analyze_image_properties(dataset_path, split='train', max_samples=100):\n",
    "    \"\"\"Analyze image properties like dimensions, file sizes, etc.\"\"\"\n",
    "    images_path = dataset_path / split / 'images'\n",
    "    \n",
    "    if not images_path.exists():\n",
    "        return None\n",
    "    \n",
    "    image_files = list(images_path.glob('*.[jp][pn]g')) + list(images_path.glob('*.jpeg'))\n",
    "    \n",
    "    # Sample images if there are too many\n",
    "    if len(image_files) > max_samples:\n",
    "        image_files = np.random.choice(image_files, max_samples, replace=False)\n",
    "    \n",
    "    properties = {\n",
    "        'widths': [],\n",
    "        'heights': [],\n",
    "        'aspect_ratios': [],\n",
    "        'file_sizes': [],\n",
    "        'formats': []\n",
    "    }\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        try:\n",
    "            # Get file size\n",
    "            file_size = img_file.stat().st_size / (1024 * 1024)  # MB\n",
    "            properties['file_sizes'].append(file_size)\n",
    "            \n",
    "            # Get image dimensions\n",
    "            with Image.open(img_file) as img:\n",
    "                width, height = img.size\n",
    "                properties['widths'].append(width)\n",
    "                properties['heights'].append(height)\n",
    "                properties['aspect_ratios'].append(width / height)\n",
    "                properties['formats'].append(img.format)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return properties\n",
    "\n",
    "# Analyze image properties for all datasets\n",
    "print(\"Analyzing image properties...\")\n",
    "image_properties = {}\n",
    "\n",
    "for dataset_path in datasets:\n",
    "    dataset_name = dataset_path.name\n",
    "    props = analyze_image_properties(dataset_path)\n",
    "    \n",
    "    if props:\n",
    "        image_properties[dataset_name] = props\n",
    "        \n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        print(f\"  Sample size: {len(props['widths'])} images\")\n",
    "        print(f\"  Width range: {min(props['widths'])} - {max(props['widths'])} px\")\n",
    "        print(f\"  Height range: {min(props['heights'])} - {max(props['heights'])} px\")\n",
    "        print(f\"  Average file size: {np.mean(props['file_sizes']):.2f} MB\")\n",
    "        print(f\"  Formats: {set(props['formats'])}\")\n",
    "    else:\n",
    "        print(f\"\\n{dataset_name}: No images found in train split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing annotations...\n"
     ]
    }
   ],
   "source": [
    "def analyze_annotations(dataset_path, split='train', max_samples=100):\n",
    "    \"\"\"Analyze YOLO format annotations.\"\"\"\n",
    "    labels_path = dataset_path / split / 'labels'\n",
    "    \n",
    "    if not labels_path.exists():\n",
    "        return None\n",
    "    \n",
    "    label_files = list(labels_path.glob('*.txt'))\n",
    "    \n",
    "    if len(label_files) > max_samples:\n",
    "        label_files = np.random.choice(label_files, max_samples, replace=False)\n",
    "    \n",
    "    annotation_stats = {\n",
    "        'objects_per_image': [],\n",
    "        'class_distribution': defaultdict(int),\n",
    "        'bbox_widths': [],\n",
    "        'bbox_heights': [],\n",
    "        'bbox_areas': [],\n",
    "        'bbox_centers_x': [],\n",
    "        'bbox_centers_y': []\n",
    "    }\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        try:\n",
    "            with open(label_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            objects_in_image = len(lines)\n",
    "            annotation_stats['objects_per_image'].append(objects_in_image)\n",
    "            \n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center = float(parts[1])\n",
    "                    y_center = float(parts[2])\n",
    "                    width = float(parts[3])\n",
    "                    height = float(parts[4])\n",
    "                    \n",
    "                    annotation_stats['class_distribution'][class_id] += 1\n",
    "                    annotation_stats['bbox_widths'].append(width)\n",
    "                    annotation_stats['bbox_heights'].append(height)\n",
    "                    annotation_stats['bbox_areas'].append(width * height)\n",
    "                    annotation_stats['bbox_centers_x'].append(x_center)\n",
    "                    annotation_stats['bbox_centers_y'].append(y_center)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {label_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return annotation_stats\n",
    "\n",
    "# Analyze annotations for all datasets\n",
    "print(\"Analyzing annotations...\")\n",
    "annotation_stats = {}\n",
    "\n",
    "for dataset_path in datasets:\n",
    "    dataset_name = dataset_path.name\n",
    "    stats = analyze_annotations(dataset_path)\n",
    "    \n",
    "    if stats:\n",
    "        annotation_stats[dataset_name] = stats\n",
    "        \n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        print(f\"  Sample size: {len(stats['objects_per_image'])} images\")\n",
    "        print(f\"  Objects per image: {np.mean(stats['objects_per_image']):.1f} ± {np.std(stats['objects_per_image']):.1f}\")\n",
    "        print(f\"  Total objects: {sum(stats['objects_per_image'])}\")\n",
    "        print(f\"  Class distribution: {dict(stats['class_distribution'])}\")\n",
    "        print(f\"  Average bbox area: {np.mean(stats['bbox_areas']):.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n{dataset_name}: No annotations found in train split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No image properties data to visualize.\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive visualizations\n",
    "if image_properties:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Image Properties Analysis Across Datasets', fontsize=16)\n",
    "    \n",
    "    # Collect data for plotting\n",
    "    all_widths = []\n",
    "    all_heights = []\n",
    "    all_aspect_ratios = []\n",
    "    all_file_sizes = []\n",
    "    dataset_labels = []\n",
    "    \n",
    "    for dataset_name, props in image_properties.items():\n",
    "        all_widths.extend(props['widths'])\n",
    "        all_heights.extend(props['heights'])\n",
    "        all_aspect_ratios.extend(props['aspect_ratios'])\n",
    "        all_file_sizes.extend(props['file_sizes'])\n",
    "        dataset_labels.extend([dataset_name] * len(props['widths']))\n",
    "    \n",
    "    # Create DataFrame for easier plotting\n",
    "    df = pd.DataFrame({\n",
    "        'width': all_widths,\n",
    "        'height': all_heights,\n",
    "        'aspect_ratio': all_aspect_ratios,\n",
    "        'file_size': all_file_sizes,\n",
    "        'dataset': dataset_labels\n",
    "    })\n",
    "    \n",
    "    # Plot 1: Width distribution\n",
    "    sns.histplot(data=df, x='width', hue='dataset', ax=axes[0,0], alpha=0.7)\n",
    "    axes[0,0].set_title('Image Width Distribution')\n",
    "    axes[0,0].set_xlabel('Width (pixels)')\n",
    "    \n",
    "    # Plot 2: Height distribution\n",
    "    sns.histplot(data=df, x='height', hue='dataset', ax=axes[0,1], alpha=0.7)\n",
    "    axes[0,1].set_title('Image Height Distribution')\n",
    "    axes[0,1].set_xlabel('Height (pixels)')\n",
    "    \n",
    "    # Plot 3: Aspect ratio distribution\n",
    "    sns.histplot(data=df, x='aspect_ratio', hue='dataset', ax=axes[0,2], alpha=0.7)\n",
    "    axes[0,2].set_title('Aspect Ratio Distribution')\n",
    "    axes[0,2].set_xlabel('Width/Height Ratio')\n",
    "    \n",
    "    # Plot 4: File size distribution\n",
    "    sns.histplot(data=df, x='file_size', hue='dataset', ax=axes[1,0], alpha=0.7)\n",
    "    axes[1,0].set_title('File Size Distribution')\n",
    "    axes[1,0].set_xlabel('File Size (MB)')\n",
    "    \n",
    "    # Plot 5: Width vs Height scatter\n",
    "    sns.scatterplot(data=df, x='width', y='height', hue='dataset', ax=axes[1,1], alpha=0.6)\n",
    "    axes[1,1].set_title('Width vs Height')\n",
    "    axes[1,1].set_xlabel('Width (pixels)')\n",
    "    axes[1,1].set_ylabel('Height (pixels)')\n",
    "    \n",
    "    # Plot 6: Dataset summary\n",
    "    dataset_summary = df.groupby('dataset').agg({\n",
    "        'width': 'count',\n",
    "        'file_size': 'mean'\n",
    "    }).reset_index()\n",
    "    dataset_summary.columns = ['dataset', 'image_count', 'avg_file_size']\n",
    "    \n",
    "    bars = axes[1,2].bar(dataset_summary['dataset'], dataset_summary['image_count'])\n",
    "    axes[1,2].set_title('Images per Dataset (Sample)')\n",
    "    axes[1,2].set_ylabel('Number of Images')\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No image properties data to visualize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation visualization\n",
    "if annotation_stats:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Annotation Analysis Across Datasets', fontsize=16)\n",
    "    \n",
    "    # Collect annotation data\n",
    "    all_objects_per_image = []\n",
    "    all_bbox_areas = []\n",
    "    all_bbox_widths = []\n",
    "    all_bbox_heights = []\n",
    "    annotation_dataset_labels = []\n",
    "    \n",
    "    for dataset_name, stats in annotation_stats.items():\n",
    "        all_objects_per_image.extend(stats['objects_per_image'])\n",
    "        all_bbox_areas.extend(stats['bbox_areas'])\n",
    "        all_bbox_widths.extend(stats['bbox_widths'])\n",
    "        all_bbox_heights.extend(stats['bbox_heights'])\n",
    "        annotation_dataset_labels.extend([dataset_name] * len(stats['objects_per_image']))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    ann_df = pd.DataFrame({\n",
    "        'objects_per_image': all_objects_per_image,\n",
    "        'dataset': annotation_dataset_labels\n",
    "    })\n",
    "    \n",
    "    bbox_df = pd.DataFrame({\n",
    "        'bbox_area': all_bbox_areas,\n",
    "        'bbox_width': all_bbox_widths,\n",
    "        'bbox_height': all_bbox_heights,\n",
    "        'dataset': [label for dataset_name, stats in annotation_stats.items() \n",
    "                   for label in [dataset_name] * len(stats['bbox_areas'])]\n",
    "    })\n",
    "    \n",
    "    # Plot 1: Objects per image\n",
    "    sns.histplot(data=ann_df, x='objects_per_image', hue='dataset', ax=axes[0,0], alpha=0.7)\n",
    "    axes[0,0].set_title('Objects per Image Distribution')\n",
    "    axes[0,0].set_xlabel('Number of Objects')\n",
    "    \n",
    "    # Plot 2: Bounding box areas\n",
    "    sns.histplot(data=bbox_df, x='bbox_area', hue='dataset', ax=axes[0,1], alpha=0.7)\n",
    "    axes[0,1].set_title('Bounding Box Area Distribution')\n",
    "    axes[0,1].set_xlabel('Normalized Area')\n",
    "    \n",
    "    # Plot 3: Bounding box width vs height\n",
    "    sns.scatterplot(data=bbox_df, x='bbox_width', y='bbox_height', hue='dataset', ax=axes[0,2], alpha=0.6)\n",
    "    axes[0,2].set_title('Bounding Box Dimensions')\n",
    "    axes[0,2].set_xlabel('Normalized Width')\n",
    "    axes[0,2].set_ylabel('Normalized Height')\n",
    "    \n",
    "    # Plot 4: Objects per image by dataset\n",
    "    sns.boxplot(data=ann_df, x='dataset', y='objects_per_image', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Objects per Image by Dataset')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 5: Class distribution summary\n",
    "    class_dist_data = []\n",
    "    for dataset_name, stats in annotation_stats.items():\n",
    "        for class_id, count in stats['class_distribution'].items():\n",
    "            class_dist_data.append({'dataset': dataset_name, 'class_id': class_id, 'count': count})\n",
    "    \n",
    "    if class_dist_data:\n",
    "        class_df = pd.DataFrame(class_dist_data)\n",
    "        sns.barplot(data=class_df, x='dataset', y='count', hue='class_id', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Class Distribution by Dataset')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 6: Total annotations summary\n",
    "    total_annotations = []\n",
    "    for dataset_name, stats in annotation_stats.items():\n",
    "        total_annotations.append({\n",
    "            'dataset': dataset_name,\n",
    "            'total_objects': sum(stats['objects_per_image'])\n",
    "        })\n",
    "    \n",
    "    if total_annotations:\n",
    "        total_df = pd.DataFrame(total_annotations)\n",
    "        bars = axes[1,2].bar(total_df['dataset'], total_df['total_objects'])\n",
    "        axes[1,2].set_title('Total Objects by Dataset')\n",
    "        axes[1,2].set_ylabel('Total Object Count')\n",
    "        axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No annotation data to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_images(dataset_path, num_samples=4):\n",
    "    \"\"\"Visualize sample images with their annotations.\"\"\"\n",
    "    images_path = dataset_path / 'train' / 'images'\n",
    "    labels_path = dataset_path / 'train' / 'labels'\n",
    "    \n",
    "    if not (images_path.exists() and labels_path.exists()):\n",
    "        print(f\"Could not find images or labels for {dataset_path.name}\")\n",
    "        return\n",
    "    \n",
    "    image_files = list(images_path.glob('*.[jp][pn]g')) + list(images_path.glob('*.jpeg'))\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(f\"No images found in {dataset_path.name}\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    samples = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    fig.suptitle(f'Sample Images from {dataset_path.name}', fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_file in enumerate(samples):\n",
    "        if i >= 4:  # Only show 4 samples\n",
    "            break\n",
    "            \n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_file))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        # Load corresponding annotation\n",
    "        label_file = labels_path / (img_file.stem + '.txt')\n",
    "        \n",
    "        if label_file.exists():\n",
    "            with open(label_file, 'r') as f:\n",
    "                annotations = f.readlines()\n",
    "            \n",
    "            # Draw bounding boxes\n",
    "            for annotation in annotations:\n",
    "                parts = annotation.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center = float(parts[1]) * width\n",
    "                    y_center = float(parts[2]) * height\n",
    "                    bbox_width = float(parts[3]) * width\n",
    "                    bbox_height = float(parts[4]) * height\n",
    "                    \n",
    "                    # Convert to corner coordinates\n",
    "                    x1 = int(x_center - bbox_width/2)\n",
    "                    y1 = int(y_center - bbox_height/2)\n",
    "                    x2 = int(x_center + bbox_width/2)\n",
    "                    y2 = int(y_center + bbox_height/2)\n",
    "                    \n",
    "                    # Draw rectangle\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    cv2.putText(img, f'Class {class_id}', (x1, y1-10), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'{img_file.name}\\n{width}x{height} px')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(samples), 4):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples from each dataset\n",
    "for dataset_path in datasets[:2]:  # Show samples from first 2 datasets to avoid too many plots\n",
    "    visualize_sample_images(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\"*70)\n",
    "print(\"CATTLE DETECTION DATASET ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_images = sum([analysis['total_images'] for analysis in dataset_analyses])\n",
    "total_annotations = sum([analysis['total_annotations'] for analysis in dataset_analyses])\n",
    "\n",
    "print(f\"\\n📊 OVERALL STATISTICS:\")\n",
    "print(f\"   • Total datasets: {len(dataset_analyses)}\")\n",
    "print(f\"   • Total images: {total_images:,}\")\n",
    "print(f\"   • Total annotations: {total_annotations:,}\")\n",
    "\n",
    "if image_properties:\n",
    "    all_widths = [w for props in image_properties.values() for w in props['widths']]\n",
    "    all_heights = [h for props in image_properties.values() for h in props['heights']]\n",
    "    all_aspects = [ar for props in image_properties.values() for ar in props['aspect_ratios']]\n",
    "    \n",
    "    print(f\"\\n🖼️  IMAGE PROPERTIES:\")\n",
    "    print(f\"   • Width range: {min(all_widths)} - {max(all_widths)} pixels\")\n",
    "    print(f\"   • Height range: {min(all_heights)} - {max(all_heights)} pixels\")\n",
    "    print(f\"   • Most common aspect ratio: {np.median(all_aspects):.2f}\")\n",
    "\n",
    "if annotation_stats:\n",
    "    all_objects = [obj for stats in annotation_stats.values() for obj in stats['objects_per_image']]\n",
    "    all_areas = [area for stats in annotation_stats.values() for area in stats['bbox_areas']]\n",
    "    \n",
    "    print(f\"\\n🎯 ANNOTATION STATISTICS:\")\n",
    "    print(f\"   • Average objects per image: {np.mean(all_objects):.1f}\")\n",
    "    print(f\"   • Max objects in single image: {max(all_objects)}\")\n",
    "    print(f\"   • Average bounding box area: {np.mean(all_areas):.3f} (normalized)\")\n",
    "\n",
    "print(f\"\\n✅ RECOMMENDATIONS FOR PHASE 1.1:\")\n",
    "print(f\"   1. Data Quality: All datasets appear to have proper YOLO format\")\n",
    "print(f\"   2. Image Diversity: Multiple datasets provide good variety\")\n",
    "print(f\"   3. Resolution: Images have varying resolutions - consider standardization\")\n",
    "print(f\"   4. Class Balance: Review class distribution for potential imbalances\")\n",
    "print(f\"   5. Next Steps: Ready to proceed with model training preparation\")\n",
    "\n",
    "print(f\"\\n🚀 READY FOR PHASE 2: Detection Model Training\")\n",
    "print(f\"   • Dataset preparation: ✅ Complete\")\n",
    "print(f\"   • Data analysis: ✅ Complete\")\n",
    "print(f\"   • Visualization: ✅ Complete\")\n",
    "print(f\"   • Next: Configure YOLO training pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "import json\n",
    "\n",
    "# Prepare data for saving (convert numpy types to native Python types)\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Create summary report\n",
    "analysis_report = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'dataset_analyses': convert_numpy_types(dataset_analyses),\n",
    "    'image_properties_summary': {},\n",
    "    'annotation_stats_summary': {},\n",
    "    'total_statistics': {\n",
    "        'total_datasets': len(dataset_analyses),\n",
    "        'total_images': total_images,\n",
    "        'total_annotations': total_annotations\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add image properties summary\n",
    "if image_properties:\n",
    "    for dataset_name, props in image_properties.items():\n",
    "        analysis_report['image_properties_summary'][dataset_name] = {\n",
    "            'sample_size': len(props['widths']),\n",
    "            'avg_width': float(np.mean(props['widths'])),\n",
    "            'avg_height': float(np.mean(props['heights'])),\n",
    "            'avg_aspect_ratio': float(np.mean(props['aspect_ratios'])),\n",
    "            'avg_file_size_mb': float(np.mean(props['file_sizes']))\n",
    "        }\n",
    "\n",
    "# Add annotation stats summary\n",
    "if annotation_stats:\n",
    "    for dataset_name, stats in annotation_stats.items():\n",
    "        analysis_report['annotation_stats_summary'][dataset_name] = {\n",
    "            'sample_size': len(stats['objects_per_image']),\n",
    "            'avg_objects_per_image': float(np.mean(stats['objects_per_image'])),\n",
    "            'total_objects': int(sum(stats['objects_per_image'])),\n",
    "            'class_distribution': convert_numpy_types(dict(stats['class_distribution'])),\n",
    "            'avg_bbox_area': float(np.mean(stats['bbox_areas']))\n",
    "        }\n",
    "\n",
    "# Save to file\n",
    "output_file = PROJECT_ROOT / 'data' / 'dataset_analysis_report.json'\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(analysis_report, f, indent=2)\n",
    "\n",
    "print(f\"Analysis report saved to: {output_file}\")\n",
    "print(f\"Report contains {len(analysis_report)} main sections\")\n",
    "print(\"This report can be used for:\")\n",
    "print(\"  • Model training configuration\")\n",
    "print(\"  • Data preprocessing decisions\")\n",
    "print(\"  • Performance benchmarking\")\n",
    "print(\"  • Documentation and reporting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
